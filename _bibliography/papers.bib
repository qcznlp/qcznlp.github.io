---
---

% =============================================================================
% 2026
% =============================================================================

@misc{xuan2026confidencedichotomyanalyzingmitigating,
  abbr      = {Preprint},
  title     = {The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents},
  author    = {Weihao Xuan and Qingcheng Zeng and Heli Qi and Yunze Xiao and Junjue Wang and Naoto Yokoya},
  year      = {2026},
  eprint    = {2601.07264},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url       = {https://arxiv.org/abs/2601.07264},
  selected  = {true}
}

@misc{yu2026pragmaticmindmachinestracing,
  abbr      = {EACL 2026},
  title     = {The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models},
  author    = {Kefan Yu and Qingcheng Zeng and Weihao Xuan and Wanxin Li and Jingyi Wu and Rob Voigt},
  year      = {2026},
  eprint    = {2505.18497},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url       = {https://arxiv.org/abs/2505.18497},
  selected  = {true}
}

@misc{guo2026deepsieveinformationsievingllmasaknowledgerouter,
  abbr      = {Findings of EACL 2026},
  title     = {DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router},
  author    = {Minghao Guo and Qingcheng Zeng and Xujiang Zhao and Yanchi Liu and Wenchao Yu and Mengnan Du and Haifeng Chen and Wei Cheng},
  year      = {2026},
  eprint    = {2507.22050},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url       = {https://arxiv.org/abs/2507.22050},
  selected  = {false}
}

@misc{liu2026naaclnoiseawareverbalconfidence,
  abbr      = {Preprint},
  title     = {NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems},
  author    = {Jiayu Liu and Rui Wang and Qing Zong and Qingcheng Zeng and Tianshi Zheng and Haochen Shi and Dadi Guo and Baixuan Xu and Chunyang Li and Yangqiu Song},
  year      = {2026},
  eprint    = {2601.11004},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url       = {https://arxiv.org/abs/2601.11004},
  selected  = {false}
}

@misc{yang2026globallargelanguagemodels,
  abbr      = {Preprint},
  title     = {Toward Global Large Language Models in Medicine},
  author    = {Rui Yang and Huitao Li and Weihao Xuan and Heli Qi and Xin Li and Kunyu Yu and Yingjian Chen and Rongrong Wang and Jacques Behmoaras and Tianxi Cai and Bibhas Chakraborty and Qingyu Chen and Lionel Tim-Ee Cheng and Marie-Louise Damwanza and Chido Dzinotyiwei and Aosong Feng and Chuan Hong and Yusuke Iwasawa and Yuhe Ke and Linah Kitala and Taehoon Ko and Jisan Lee and Irene Li and Jonathan Chong Kai Liew and Hongfang Liu and Lian Leng Low and Edison Marrese-Taylor and Yutaka Matsuo and Isheanesu Misi and Yilin Ning and Jasmine Chiat Ling Ong and Marcus Eng Hock Ong and Enrico Petretto and Hossein Rouhizadeh and Abiram Sandralegar and Oren Schreier and Iain Bee Huat Tan and Patrick Tan and Daniel Shu Wei Ting and Junjue Wang and Chunhua Weng and Matthew Yu Heng Wong and Fang Wu and Yunze Xiao and Xuhai Xu and Qingcheng Zeng and Zhuo Zheng and Yifan Peng and Douglas Teodoro and Nan Liu},
  year      = {2026},
  eprint    = {2601.02186},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url       = {https://arxiv.org/abs/2601.02186},
  selected  = {false}
}

% =============================================================================
% 2025
% =============================================================================

@inproceedings{zeng-etal-2025-thinking,
  abbr      = {EMNLP 2025},
  title     = {Thinking Out Loud: Do Reasoning Models Know When They're Right?},
  author    = {Zeng, Qingcheng and Xuan, Weihao and Cui, Leyang and Voigt, Rob},
  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2025},
  address   = {Suzhou, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.emnlp-main.73/},
  doi       = {10.18653/v1/2025.emnlp-main.73},
  pages     = {1394--1407},
  abstract  = {Large reasoning models (LRMs) have recently demonstrated impressive capabilities in complex reasoning tasks by leveraging increased test-time computation and exhibiting behaviors reminiscent of human-like self-reflection. While LRMs show a clear capacity for valuable self-reflection, how this ability interacts with other model behaviors remains underexplored. We investigate this connection by analyzing verbalized confidence, how models articulate their certainty, as a lens into the nature of self-reflection in LRMs. We find that supervised fine-tuning on reasoning traces (i.e., distillation) and reinforcement learning can improve verbalized calibration in reasoning-intensive settings in a progressive, laddered fashion. However, our results also indicate that reasoning models may possess a diminished awareness of their own knowledge boundaries, as evidenced by significantly lower "I don't know" response rates on factuality benchmarks. Moreover, we examine the relationship between verbalized confidence and reasoning chains, finding that models tend to express higher confidence when providing shorter or less elaborate reasoning. Our findings highlight how reasoning-oriented training can enhance performance in reasoning-centric tasks while potentially incurring a reasoning tax, a cost reflected in the model's reduced ability to accurately recognize the limits of its own knowledge in small-scale models. More broadly, our work showcases how this erosion of knowledge boundaries can compromise model faithfulness, as models grow more confident without a commensurate understanding of when they should abstain.},
  selected  = {true}
}

@inproceedings{xuan-etal-2025-seeing,
  abbr      = {EMNLP 2025},
  title     = {Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models},
  author    = {Xuan, Weihao and Zeng, Qingcheng and Qi, Heli and Wang, Junjue and Yokoya, Naoto},
  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2025},
  address   = {Suzhou, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.emnlp-main.74/},
  doi       = {10.18653/v1/2025.emnlp-main.74},
  pages     = {1408--1450},
  abstract  = {Uncertainty quantification is essential for assessing the reliability and trustworthiness of modern AI systems. Among existing approaches, verbalized uncertainty, where models express their confidence through natural language, has emerged as a lightweight and interpretable solution in large language models (LLMs). However, its effectiveness in vision-language models (VLMs) remains insufficiently studied. In this work, we conduct a comprehensive evaluation of verbalized confidence in VLMs, spanning three model categories, four task domains, and three evaluation scenarios. Our results show that current VLMs often display notable miscalibration across diverse tasks and settings. Notably, visual reasoning models (i.e., thinking with images) consistently exhibit better calibration, suggesting that modality-specific reasoning is critical for reliable uncertainty estimation. To further address calibration challenges, we introduce Visual Confidence-Aware Prompting, a two-stage prompting strategy that improves confidence alignment in multimodal settings. Overall, our study highlights the inherent miscalibration in VLMs across modalities. More broadly, our findings underscore the fundamental importance of modality alignment and model faithfulness in advancing reliable multimodal systems.},
  selected  = {true}
}

@inproceedings{xuan-etal-2025-mmlu,
  abbr      = {EMNLP 2025},
  title     = {MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation},
  author    = {Xuan, Weihao and Yang, Rui and Qi, Heli and Zeng, Qingcheng and Xiao, Yunze and Feng, Aosong and Liu, Dairui and Xing, Yun and Wang, Junjue and Gao, Fan and Lu, Jinghui and Jiang, Yuang and Li, Huitao and Li, Xin and Yu, Kunyu and Dong, Ruihai and Gu, Shangding and Li, Yuekang and Xie, Xiaofei and Juefei-Xu, Felix and Khomh, Foutse and Yoshie, Osamu and Chen, Qingyu and Teodoro, Douglas and Liu, Nan and Goebel, Randy and Ma, Lei and Marrese-Taylor, Edison and Lu, Shijian and Iwasawa, Yusuke and Matsuo, Yutaka and Li, Irene},
  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2025},
  address   = {Suzhou, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.emnlp-main.79/},
  doi       = {10.18653/v1/2025.emnlp-main.79},
  pages     = {1513--1532},
  abstract  = {Existing large language model (LLM) evaluation benchmarks primarily focus on English, while current multilingual tasks lack parallel questions that specifically assess cross-lingual reasoning abilities. This dual limitation makes it challenging to assess LLMs' performance in the multilingual setting comprehensively. To fill this gap, we introduce MMLU-ProX, a comprehensive benchmark covering 29 languages, built on an English benchmark. Each language version consists of 11,829 identical questions, enabling direct cross-lingual comparisons. Additionally, to meet efficient evaluation needs, we provide a lite version containing 658 questions per language. To ensure the high quality of MMLU-ProX, we employ a rigorous development process that involves multiple powerful LLMs for translation, followed by expert review to ensure accurate expression, consistent terminology, and cultural relevance. Building on this, we systematically evaluate 36 state-of-the-art LLMs, including reasoning-enhanced and multilingual-optimized LLMs. The results reveal significant disparities in the multilingual capabilities of LLMs: While they perform well in high-resource languages, their performance declines markedly in low-resource languages, particularly for African languages. Through MMLU-ProX, we aim to advance the development of more inclusive AI systems and promote equitable access to technology across global contexts.},
  selected  = {false}
}

@inproceedings{lefevre-etal-2025-good,
  abbr      = {EMNLP 2025},
  title     = {Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?},
  author    = {LeFevre, Grace and Zeng, Qingcheng and Leif, Adam and Jewell, Jason and Peskoff, Denis and Voigt, Rob},
  booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2025},
  address   = {Suzhou, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.emnlp-main.259/},
  doi       = {10.18653/v1/2025.emnlp-main.259},
  pages     = {5138--5150},
  abstract  = {The social impact of Natural Language Processing (NLP) is increasingly important, with a rising community focus on initiatives related to NLP for Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the ACL Anthology address topics related to social good as defined by the UN Sustainable Development Goals (Aduato et al. 2023). In this study, we take an author- and venue-level perspective to map the landscape of NLP4SG, quantifying the proportion of work addressing social good concerns both within and beyond the ACL community, by both core ACL contributors and non-ACL authors. With this approach we discover two surprising facts about the landscape of NLP4SG. First, ACL authors are dramatically more likely to do work addressing social good concerns when publishing in venues outside of ACL. Second, the vast majority of publications using NLP techniques to address concerns of social good are done by non-ACL authors in venues outside of ACL. We discuss the implications of these findings on agenda-setting considerations for the ACL community related to NLP4SG.},
  selected  = {false}
}

@inproceedings{10.1145/3746252.3760887,
  abbr      = {CIKM 2025},
  title     = {Uncertainty Quantification for Multiple-Choice Questions is Just One-Token Deep},
  author    = {Zeng, Qingcheng and Jin, Mingyu and Yu, Qinkai and Wang, Zhenting and Hua, Wenyue and Sun, Guangyan and Meng, Yanda and Ma, Shiqing and Wang, Qifan and Juefei-Xu, Felix and Yang, Fan and Ding, Kaize and Tang, Ruixiang and Zhang, Yongfeng},
  booktitle = {Proceedings of the 34th ACM International Conference on Information and Knowledge Management},
  year      = {2025},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3746252.3760887},
  doi       = {10.1145/3746252.3760887},
  pages     = {5474--5478},
  location  = {Seoul, Republic of Korea},
  series    = {CIKM '25},
  abstract  = {Multiple-choice question (MCQ) benchmarks such as MMLU and GPQA are widely used to assess the capabilities of large language models (LLMs). While accuracy remains the standard evaluation metric, recent work has introduced uncertainty quantification (UQ) methods, such as entropy, conformal prediction, and verbalized confidence, as complementary measures of model reliability and calibration. However, we find that these UQ methods, when applied to MCQ tasks, are unexpectedly fragile. Specifically, we show that fine-tuning a model on just 1,000 examples to adjust the probability of the first generated token, under the common prompting setup where the model is instructed to output only a single answer choice, can systematically distort a broad range of UQ methods across models, prompts, and domains, all while leaving answer accuracy unchanged. We validate this phenomenon through extensive experiments on five instruction-tuned LLMs, tested under standard prompting, zero-shot chain-of-thought reasoning, and a biomedical question answering setting. In all cases, models retain similar accuracy but exhibit significantly degraded calibration. These results suggest that current UQ practices for MCQs are "one-token deep", driven more by first-token decoding behavior than by any deeper representation of uncertainty, and are easily manipulated through minimal interventions. Our findings call for more robust and interpretable approaches to uncertainty estimation, particularly in structured formats like MCQs, where confidence signals are often reduced to token-level heuristics.},
  selected  = {true}
}

@inproceedings{10.1145/3746252.3760818,
  abbr      = {CIKM 2025},
  title     = {Fact or Facsimile? Evaluating the Factual Robustness of Modern Retrievers},
  author    = {Wu, Haoyu and Zeng, Qingcheng and Ding, Kaize},
  booktitle = {Proceedings of the 34th ACM International Conference on Information and Knowledge Management},
  year      = {2025},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3746252.3760818},
  doi       = {10.1145/3746252.3760818},
  pages     = {5351--5355},
  location  = {Seoul, Republic of Korea},
  series    = {CIKM '25},
  abstract  = {Dense retrievers and rerankers are central to retrieval-augmented generation (RAG) pipelines, where accurately retrieving factual information is crucial for maintaining system trustworthiness and defending against RAG poisoning. However, little is known about how much factual competence these components inherit or lose from the large language models (LLMs) they are based on. We pair 12 publicly released embedding checkpoints with their original base LLMs and evaluate both sets on a factuality benchmark. Across every model evaluated, the embedding variants achieve markedly lower accuracy than their bases, with absolute drops ranging from 12 to 43 percentage points (median 28 pts) and typical retriever accuracies collapsing into the 25-35% band versus the 60-70% attained by the generative models. This degradation intensifies under a more demanding condition: when the candidate pool per question is expanded from four options to one thousand, the strongest retriever's top-1 accuracy falls from 33% to 26%, revealing acute sensitivity to distractor volume. Statistical tests further show that, for every embedding model, cosine-similarity scores between queries and correct completions are significantly higher than those for incorrect ones (p < 0.01), indicating decisions driven largely by surface-level semantic proximity rather than factual reasoning. To probe this weakness, we employed GPT-4.1 to paraphrase each correct completion, creating a rewritten test set that preserved factual truth while masking lexical cues, and observed that over two-thirds of previously correct predictions flipped to wrong, reducing overall accuracy to roughly one-third of its original level. Taken together, these findings reveal a systematic trade-off introduced by contrastive learning for retrievers: gains in semantic retrieval are paid for with losses in parametric factual knowledge, and the resulting models remain highly vulnerable to adversarial or even benign rephrasings. Our study underscores the need for retrieval objectives that balance similarity with factual fidelity to safeguard next-generation RAG systems against both misinformation and targeted attacks.},
  selected  = {true}
}

@inproceedings{lam-etal-2025-leveraging,
  abbr      = {ACL 2025},
  title     = {Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility},
  author    = {Lam, Suet-Ying and Zeng, Qingcheng and Wu, Jingyi and Voigt, Rob},
  booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = jul,
  year      = {2025},
  address   = {Vienna, Austria},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.acl-short.14/},
  doi       = {10.18653/v1/2025.acl-short.14},
  pages     = {158--171},
  abstract  = {Whether large language models (LLMs) process language similarly to humans has been the subject of much theoretical and practical debate. We examine this question through the lens of the production-interpretation distinction found in human sentence processing and evaluate the extent to which instruction-tuned LLMs replicate this distinction. Using an empirically documented asymmetry between pronoun production and interpretation in humans for implicit causality verbs as a testbed, we find that some LLMs do quantitatively and qualitatively reflect human-like asymmetries between production and interpretation. We demonstrate that whether this behavior holds depends upon both model size-with larger models more likely to reflect human-like patterns and the choice of meta-linguistic prompts used to elicit the behavior. Our codes and results are available here.},
  selected  = {false}
}

@inproceedings{li-etal-2025-exploring-multilingual,
  abbr      = {XLLM 2025},
  title     = {Exploring Multilingual Probing in Large Language Models: A Cross-Language Analysis},
  author    = {Li, Daoyang and Zhao, Haiyan and Zeng, Qingcheng and Du, Mengnan},
  booktitle = {Proceedings of the 1st Joint Workshop on Large Language Models and Structure Modeling (XLLM 2025)},
  month     = aug,
  year      = {2025},
  address   = {Vienna, Austria},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.xllm-1.7/},
  doi       = {10.18653/v1/2025.xllm-1.7},
  pages     = {61--70},
  abstract  = {Probing techniques for large language models (LLMs) have primarily focused on English, overlooking the vast majority of other world's languages. In this paper, we extend these probing methods to a multilingual context, investigating how LLMs encode linguistic structures across diverse languages. We conduct experiments on several open-source LLM models, analyzing probing accuracy, trends across layers, and similarities between probing vectors for multiple languages. Our key findings reveal: (1) a consistent performance gap between high-resource and low-resource languages, with high-resource languages achieving significantly higher probing accuracy; (2) divergent layer-wise accuracy trends, where high-resource languages show substantial improvement in deeper layers similar to English; and (3) higher representational similarities among high-resource languages, with low-resource languages demonstrating lower similarities both among themselves and with high-resource languages. These results provide insights into how linguistic structures are represented differently across languages in LLMs and emphasize the need for improved structure modeling for low-resource languages.},
  selected  = {false}
}

@inproceedings{jin-etal-2025-exploring,
  abbr      = {COLING 2025},
  title     = {Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concept at Different Layers?},
  author    = {Jin, Mingyu and Yu, Qinkai and Huang, Jingyuan and Zeng, Qingcheng and Wang, Zhenting and Hua, Wenyue and Zhao, Haiyan and Mei, Kai and Meng, Yanda and Ding, Kaize and Yang, Fan and Du, Mengnan and Zhang, Yongfeng},
  booktitle = {Proceedings of the 31st International Conference on Computational Linguistics},
  month     = jan,
  year      = {2025},
  address   = {Abu Dhabi, UAE},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.coling-main.37/},
  pages     = {558--573},
  abstract  = {Large language models (LLMs) have shown remarkable performances across a wide range of tasks. However, the mechanisms by which these models encode tasks of varying complexities remain poorly understood. In this paper, we explore the hypothesis that LLMs process concepts of varying complexities in different layers, introducing the idea of "Concept Depth" to suggest that more complex concepts are typically acquired in deeper layers. Specifically, we categorize concepts based on their level of abstraction, defining them in the order of increasing complexity within factual, emotional, and inferential tasks. We conduct extensive probing experiments using layer-wise representations across various LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the three domains of tasks. Our findings reveal that models could efficiently conduct probing for simpler tasks in shallow layers, and more complex tasks typically necessitate deeper layers for accurate understanding. Additionally, we examine how external factors, such as adding noise to the input and quantizing the model weights, might affect layer-wise representations. Our findings suggest that these factors can impede the development of a conceptual understanding of LLMs until deeper layers are explored. We hope that our proposed concept and experimental insights will enhance the understanding of the mechanisms underlying LLMs. Our codes are available at https://github.com/Luckfort/CD.},
  selected  = {false}
}

@article{doi:10.1177/13623613241304488,
  abbr      = {Autism},
  title     = {Pre-trained artificial intelligence language model represents pragmatic language variability central to autism and genetically related phenotypes},
  author    = {Joseph CY Lau and Emily Landau and Qingcheng Zeng and Ruichun Zhang and Stephanie Crawford and Rob Voigt and Molly Losh},
  journal   = {Autism},
  volume    = {29},
  number    = {5},
  pages     = {1346--1358},
  year      = {2025},
  doi       = {10.1177/13623613241304488},
  url       = {https://doi.org/10.1177/13623613241304488},
  abstract  = {Many individuals with autism experience challenges using language in social contexts (i.e., pragmatic language). Characterizing and understanding pragmatic variability is important to inform intervention strategies and the etiology of communication challenges in autism; however, current manual coding-based methods are often time and labor intensive, and not readily applied in ample sample sizes. This proof-of-concept methodological study employed an artificial intelligence pre-trained language model, Bidirectional Encoder Representations from Transformers, as a tool to address such challenges. We applied Bidirectional Encoder Representations from Transformers to computationally index pragmatic-related variability in autism and in genetically related phenotypes displaying pragmatic differences, namely, in parents of autistic individuals, fragile X syndrome, and FMR1 premutation. Findings suggest that without model fine-tuning, Bidirectional Encoder Representations from Transformers's Next Sentence Prediction module was able to derive estimates that differentiate autistic from non-autistic groups. Moreover, such computational estimates correlated with manually coded characterization of pragmatic abilities that contribute to conversational coherence, not only in autism but also in the other genetically related phenotypes. This study represents a step forward in evaluating the efficacy of artificial intelligence language models for capturing clinically important pragmatic differences and variability related to autism, showcasing the potential of artificial intelligence to provide automatized, efficient, and objective tools for pragmatic characterization to help advance the field.},
  selected  = {false}
}

@misc{zeng2025sympathypolarizationcomputationaldiscourse,
  abbr      = {Findings of AACL 2025},
  title     = {Sympathy over Polarization: A Computational Discourse Analysis of Social Media Posts about the July 2024 Trump Assassination Attempt},
  author    = {Qingcheng Zeng and Guanhong Liu and Zhaoqian Xue and Diego Ford and Rob Voigt and Loni Hagen and Lingyao Li},
  year      = {2025},
  eprint    = {2501.09950},
  archivePrefix = {arXiv},
  primaryClass = {cs.SI},
  url       = {https://arxiv.org/abs/2501.09950},
  selected  = {true}
}

@misc{tu2025positionhiddencostsmeasurement,
  abbr      = {Preprint},
  title     = {Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards},
  author    = {Aaron Tu and Weihao Xuan and Heli Qi and Xu Huang and Qingcheng Zeng and Shayan Talaei and Yijia Xiao and Peng Xia and Xiangru Tang and Yuchen Zhuang and Bing Hu and Hanqun Cao and Wenqi Shi and Tianang Leng and Rui Yang and Yingjian Chen and Ziqi Wang and Irene Li and Nan Liu and Huaxiu Yao and Li Erran Li and Ge Liu and Amin Saberi and Naoto Yokoya and Jure Leskovec and Yejin Choi and Fang Wu},
  year      = {2025},
  eprint    = {2509.21882},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url       = {https://arxiv.org/abs/2509.21882},
  selected  = {true}
}

@misc{chen2025carescomprehensiveevaluationsafety,
  abbr      = {NeurIPS 2025},
  title     = {CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs},
  author    = {Sijia Chen and Xiaomin Li and Mengxue Zhang and Eric Hanchen Jiang and Qingcheng Zeng and Chen-Hsiang Yu},
  year      = {2025},
  eprint    = {2505.11413},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url       = {https://arxiv.org/abs/2505.11413},
  selected  = {false}
}

@misc{schoenegger2025largelanguagemodelspersuasive,
  abbr      = {Preprint},
  title     = {Large Language Models Are More Persuasive Than Incentivized Human Persuaders},
  author    = {Philipp Schoenegger and Francesco Salvi and Jiacheng Liu and Xiaoli Nan and Ramit Debnath and Barbara Fasolo and Evelina Leivada and Gabriel Recchia and Fritz Günther and Ali Zarifhonarvar and Joe Kwon and Zahoor Ul Islam and Marco Dehnert and Daryl Y. H. Lee and Madeline G. Reinecke and David G. Kamper and Mert Kobaş and Adam Sandford and Jonas Kgomo and Luke Hewitt and Shreya Kapoor and Kerem Oktar and Eyup Engin Kucuk and Bo Feng and Cameron R. Jones and Izzy Gainsburg and Sebastian Olschewski and Nora Heinzelmann and Francisco Cruz and Ben M. Tappin and Tao Ma and Peter S. Park and Rayan Onyonka and Arthur Hjorth and Peter Slattery and Qingcheng Zeng and Lennart Finke and Igor Grossmann and Alessandro Salatiello and Ezra Karger},
  year      = {2025},
  eprint    = {2505.09662},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url       = {https://arxiv.org/abs/2505.09662},
  selected  = {false}
}

@misc{huang2025thinkbenchdynamicoutofdistributionevaluation,
  abbr      = {NeurIPS 2025},
  title     = {ThinkBench: Dynamic Out-of-Distribution Evaluation for Robust LLM Reasoning},
  author    = {Shulin Huang and Linyi Yang and Yan Song and Shuang Chen and Leyang Cui and Ziyu Wan and Qingcheng Zeng and Ying Wen and Kun Shao and Weinan Zhang and Jun Wang and Yue Zhang},
  year      = {2025},
  eprint    = {2502.16268},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url       = {https://arxiv.org/abs/2502.16268},
  selected  = {false}
}

@misc{liu2025veriguiverifiablelongchaingui,
  abbr      = {Preprint},
  title     = {VeriGUI: Verifiable Long-Chain GUI Dataset},
  author    = {Shunyu Liu and Minghao Liu and Huichi Zhou and Zhenyu Cui and Yang Zhou and Yuhao Zhou and Wendong Fan and Ge Zhang and Jiajun Shi and Weihao Xuan and Jiaxing Huang and Shuang Luo and Fang Wu and Heli Qi and Qingcheng Zeng and Ziqi Ren and Jialiang Gao and Jindi Lv and Junjie Wang and Aosong Feng and Heng Zhou and Wangchunshu Zhou and Zhenfei Yin and Wenlong Zhang and Guohao Li and Wenhao Yu and Irene Li and Lei Ma and Lei Bai and Qunshu Lin and Mingli Song and Dacheng Tao},
  year      = {2025},
  eprint    = {2508.04026},
  archivePrefix = {arXiv},
  primaryClass = {cs.HC},
  url       = {https://arxiv.org/abs/2508.04026},
  selected  = {false}
}

@misc{xue2025equitableaccessleveragingcrowdsourced,
  abbr      = {Preprint},
  title     = {Toward Equitable Access: Leveraging Crowdsourced Reviews to Investigate Public Perceptions of Health Resource Accessibility},
  author    = {Zhaoqian Xue and Guanhong Liu and Chong Zhang and Kai Wei and Qingcheng Zeng and Songhua Hu and Wenyue Hua and Lizhou Fan and Yongfeng Zhang and Lingyao Li},
  year      = {2025},
  eprint    = {2502.10641},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url       = {https://arxiv.org/abs/2502.10641},
  selected  = {false}
}

% =============================================================================
% 2024
% =============================================================================

@inproceedings{zeng-etal-2024-adaptive,
  abbr      = {EMNLP 2024},
  title     = {Adaptive Axes: A Pipeline for In-domain Social Stereotype Analysis},
  author    = {Zeng, Qingcheng and Jin, Mingyu and Voigt, Rob},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.emnlp-main.872/},
  doi       = {10.18653/v1/2024.emnlp-main.872},
  pages     = {15576--15593},
  abstract  = {Prior work has explored the possibility of using the semantic information obtained from embedding representations to quantify social stereotypes, leveraging techniques such as word embeddings combined with a list of traits (Garg et al., 2018; Charlesworth et al., 2022) or semantic axes (An et al., 2018; Lucy et al., 2022). However, these approaches have struggled to fully capture the variability in stereotypes across different conceptual domains for the same social group (e.g., black in science, health, and art), in part because the identity of a word and the associations formed during pre-training can dominate its contextual representation (Field and Tsvetkov, 2019). This study explores the ability to recover stereotypes from the contexts surrounding targeted entities by utilizing state-of-the-art text embedding models and adaptive semantic axes enhanced by large language models (LLMs). Our results indicate that the proposed pipeline not only surpasses token-based methods in capturing in-domain framing but also effectively tracks stereotypes over time and along domain-specific semantic axes for in-domain texts. Our research highlights the potential of employing text embedding models to achieve a deeper understanding of nuanced social stereotypes.},
  selected  = {true}
}

@inproceedings{heddaya-etal-2024-causal,
  abbr      = {WNU 2024},
  title     = {Causal Micro-Narratives},
  author    = {Heddaya, Mourad and Zeng, Qingcheng and Zentefis, Alexander and Voigt, Rob and Tan, Chenhao},
  booktitle = {Proceedings of the 6th Workshop on Narrative Understanding},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.wnu-1.12/},
  doi       = {10.18653/v1/2024.wnu-1.12},
  pages     = {67--84},
  abstract  = {We present a novel approach to classify causal micro-narratives from text. These narratives are sentence-level explanations of the cause(s) and/or effect(s) of a target subject. The approach requires only a subject-specific ontology of causes and effects, and we demonstrate it with an application to inflation narratives. Using a human-annotated dataset spanning historical and contemporary US news articles for training, we evaluate several large language models (LLMs) on this multi-label classification task. The best-performing model—a fine-tuned Llama 3.1 8B—achieves F1 scores of 0.87 on narrative detection and 0.71 on narrative classification. Comprehensive error analysis reveals challenges arising from linguistic ambiguity and highlights how model errors often mirror human annotator disagreements. This research establishes a framework for extracting causal micro-narratives from real-world data, with wide-ranging applications to social science research.},
  selected  = {true}
}

@inproceedings{yang-etal-2024-kg,
  abbr      = {BioNLP 2024},
  title     = {KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques},
  author    = {Yang, Rui and Liu, Haoran and Marrese-Taylor, Edison and Zeng, Qingcheng and Ke, Yuhe and Li, Wanxin and Cheng, Lechao and Chen, Qingyu and Caverlee, James and Matsuo, Yutaka and Li, Irene},
  booktitle = {Proceedings of the 23rd Workshop on Biomedical Natural Language Processing},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.bionlp-1.13/},
  doi       = {10.18653/v1/2024.bionlp-1.13},
  pages     = {155--166},
  abstract  = {Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) with ranking and re-ranking techniques, aiming to improve free-text question-answering (QA) in the medical domain. Specifically, upon receiving a question, we initially retrieve triplets from a medical KG to gather factual information. Subsequently, we innovatively apply ranking methods to refine the ordering of these triplets, aiming to yield more precise answers. To the best of our knowledge, KG-Rank is the first application of ranking models combined with KG in medical QA specifically for generating long answers. Evaluation of four selected medical QA datasets shows that KG-Rank achieves an improvement of over 18% in the ROUGE-L score. Moreover, we extend KG-Rank to open domains, where it realizes a 14% improvement in ROUGE-L, showing the effectiveness and potential of KG-Rank.},
  selected  = {false}
}

@inproceedings{gao-etal-2024-evaluating-large,
  abbr      = {Findings of ACL 2024},
  title     = {Evaluating Large Language Models on Wikipedia-Style Survey Generation},
  author    = {Gao, Fan and Jiang, Hang and Yang, Rui and Zeng, Qingcheng and Lu, Jinghui and Blum, Moritz and She, Tianwei and Jiang, Yuang and Li, Irene},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.findings-acl.321/},
  doi       = {10.18653/v1/2024.findings-acl.321},
  pages     = {5405--5418},
  abstract  = {Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.},
  selected  = {false}
}

@article{doi:10.1056/AIra2400012,
  abbr      = {NEJM AI},
  title     = {Clinical Text Datasets for Medical Artificial Intelligence and Large Language Models — A Systematic Review},
  author    = {Jiageng Wu and Xiaocong Liu and Minghui Li and Wanxin Li and Zichang Su and Shixu Lin and Lucas Garay and Zhiyun Zhang and Yujie Zhang and Qingcheng Zeng and Jie Shen and Changzheng Yuan and Jie Yang},
  journal   = {NEJM AI},
  volume    = {1},
  number    = {6},
  pages     = {AIra2400012},
  year      = {2024},
  doi       = {10.1056/AIra2400012},
  url       = {https://ai.nejm.org/doi/full/10.1056/AIra2400012},
  abstract  = {Privacy and ethical considerations limit access to large-scale clinical datasets, particularly clinical text data, which contain extensive and diverse information and serve as the foundation for building clinical large language models (LLMs). The limited accessibility of clinical text data impedes the development of clinical artificial intelligence systems and hampers research participation from resource-poor regions and medical institutions, thereby exacerbating health care disparities. In this review, we conduct a global review to identify publicly available clinical text datasets and elaborate on their accessibility, diversity, and usability for clinical LLMs.},
  selected  = {false}
}

@article{info:doi/10.2196/60601,
  abbr      = {JMIR},
  title     = {Ascle—A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study},
  author    = {Rui Yang and Qingcheng Zeng and Keen You and Yujie Qiao and Lucas Huang and Chia-Chun Hsieh and Benjamin Rosand and Jeremy Goldwasser and Amisha Dave and Tiarnan Keenan and Yuhe Ke and Chuan Hong and Nan Liu and Emily Chew and Dragomir Radev and Zhiyong Lu and Hua Xu and Qingyu Chen and Irene Li},
  journal   = {J Med Internet Res},
  year      = {2024},
  month     = oct,
  volume    = {26},
  pages     = {e60601},
  doi       = {10.2196/60601},
  url       = {https://www.jmir.org/2024/1/e60601},
  abstract  = {This study aims to describe the development and preliminary evaluation of Ascle. Ascle is tailored for biomedical researchers and clinical staff with an easy-to-use, all-in-one solution that requires minimal programming expertise. For the first time, Ascle provides 4 advanced and challenging generative functions: question-answering, text summarization, text simplification, and machine translation. In addition, Ascle integrates 12 essential NLP functions, along with query and search capabilities for clinical databases.},
  selected  = {true}
}

% =============================================================================
% 2023
% =============================================================================

@inproceedings{10.24963/ijcai.2023/698,
  abbr      = {IJCAI 2023},
  title     = {GreenPLM: Cross-lingual Transfer of Monolingual Pre-trained Language Models at Almost No Cost},
  author    = {Zeng, Qingcheng and Garay, Lucas and Zhou, Peilin and Chong, Dading and Hua, Yining and Wu, Jiageng and Pan, Yikang and Zhou, Han and Voigt, Rob and Yang, Jie},
  booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
  year      = {2023},
  url       = {https://doi.org/10.24963/ijcai.2023/698},
  doi       = {10.24963/ijcai.2023/698},
  pages     = {6246--6254},
  location  = {Macao, P.R.China},
  series    = {IJCAI '23},
  abstract  = {Large pre-trained models have revolutionized natural language processing (NLP) research and applications, but high training costs and limited data resources have prevented their benefits from being shared equally amongst speakers of all the world's languages. To address issues of cross-linguistic access to such models and reduce energy consumption for sustainability during large-scale model training, this study proposes an effective and energy-efficient framework called Green-PLM that uses bilingual lexicons to directly "translate" pre-trained language models of one language into another at almost no additional cost. We validate this approach in 18 languages' BERT models and show that this framework is comparable to, if not better than, other heuristics with high training costs. In addition, given lightweight continued pretraining on limited data where available, this framework outperforms the original monolingual language models in six out of seven tested languages with up to 200x less pre-training efforts. Aiming at the Leave No One Behind Principle (LNOB), our approach manages to reduce inequalities between languages and energy consumption greatly. We make our codes and models publicly available at https://github.com/qcznlp/GreenPLMs.},
  selected  = {true}
}

@inproceedings{lam-etal-2023-large,
  abbr      = {Findings of ACL 2023},
  title     = {Large Language Models Are Partially Primed in Pronoun Interpretation},
  author    = {Lam, Suet-Ying and Zeng, Qingcheng and Zhang, Kexun and You, Chenyu and Voigt, Rob},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.605/},
  doi       = {10.18653/v1/2023.findings-acl.605},
  pages     = {9493--9506},
  abstract  = {While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way. The present study probes this question by asking whether LLMs display human-like referential biases using stimuli and procedures from real psycholinguistic experiments. Recent psycholinguistic studies suggest that humans adapt their referential biases with recent exposure to referential patterns; closely replicating three relevant psycholinguistic experiments from Johnson & Arnold (2022) in an in-context learning (ICL) framework, we found that InstructGPT adapts its pronominal interpretations in response to the frequency of referential patterns in the local discourse, though in a limited fashion: adaptation was only observed relative to syntactic but not semantic biases. By contrast, FLAN-UL2 fails to generate meaningful patterns. Our results provide further evidence that contemporary LLMs discourse representations are sensitive to syntactic patterns in the local context but less so to semantic patterns. Our data and code are available at https://github.com/zkx06111/llm_priming.},
  selected  = {false}
}

@inproceedings{10095691,
  abbr      = {ICASSP 2023},
  title     = {Masked Spectrogram Prediction for Self-Supervised Audio Pre-Training},
  author    = {Chong, Dading and Wang, Helin and Zhou, Peilin and Zeng, Qingcheng},
  booktitle = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2023},
  pages     = {1--5},
  doi       = {10.1109/ICASSP49357.2023.10095691},
  selected  = {false}
}

% =============================================================================
% 2022
% =============================================================================

@inproceedings{zeng-li-2022-survey,
  abbr      = {COLING 2022},
  title     = {A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-X Perspectives},
  author    = {Zeng, Qingcheng and Li, An-Ran},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
  month     = oct,
  year      = {2022},
  address   = {Gyeongju, Republic of Korea},
  publisher = {International Committee on Computational Linguistics},
  url       = {https://aclanthology.org/2022.coling-1.69/},
  pages     = {824--836},
  abstract  = {Irony is a ubiquitous figurative language in daily communication. Previously, many researchers have approached irony from linguistic, cognitive science, and computational aspects. Recently, some progress have been witnessed in automatic irony processing due to the rapid development in deep neural models in natural language processing (NLP). In this paper, we will provide a comprehensive overview of computational irony, insights from linguisic theory and cognitive science, as well as its interactions with downstream NLP tasks and newly proposed multi-X irony processing perspectives.},
  selected  = {false}
}

@inproceedings{zeng22b_interspeech,
  abbr      = {Interspeech 2022},
  title     = {Low-resource Accent Classification in Geographically-proximate Settings: A Forensic and Sociophonetics Perspective},
  author    = {Qingcheng Zeng and Dading Chong and Peilin Zhou and Jie Yang},
  booktitle = {Interspeech 2022},
  year      = {2022},
  pages     = {5308--5312},
  doi       = {10.21437/Interspeech.2022-11372},
  selected  = {false}
}

@inproceedings{zhou22b_interspeech,
  abbr      = {Interspeech 2022},
  title     = {Calibrate and Refine! A Novel and Agile Framework for ASR Error Robust Intent Detection},
  author    = {Peilin Zhou and Dading Chong and Helin Wang and Qingcheng Zeng},
  booktitle = {Interspeech 2022},
  year      = {2022},
  pages     = {1096--1100},
  doi       = {10.21437/Interspeech.2022-786},
  selected  = {false}
}

@misc{jin2022filterevolveprogressivepseudo,
  abbr      = {Preprint},
  title     = {Filter and Evolve: Progressive Pseudo Label Refining for Semi-Supervised Automatic Speech Recognition},
  author    = {Zezhong Jin and Dading Zhong and Xiao Song and Zhaoyi Liu and Naipeng Ye and Qingcheng Zeng},
  year      = {2022},
  eprint    = {2210.16318},
  archivePrefix = {arXiv},
  primaryClass = {cs.SD},
  url       = {https://arxiv.org/abs/2210.16318},
  selected  = {false}
}
